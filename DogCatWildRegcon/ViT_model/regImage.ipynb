{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f510d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import display\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1bc5980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "970645db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttendtion (nn.Module):\n",
    "    def __init__(self,embed_dim,num_heads,qkv_bias=True):\n",
    "        super(MultiHeadAttendtion, self).__init__()\n",
    "        self.num_head = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.scale = embed_dim**-0.5\n",
    "\n",
    "        self.query = nn.Conv1d(in_channels=embed_dim,out_channels=embed_dim,kernel_size=1,bias=qkv_bias)\n",
    "        self.key = nn.Conv1d(in_channels=embed_dim,out_channels=embed_dim,kernel_size=1,bias=qkv_bias)\n",
    "        self.value = nn.Conv1d(in_channels=embed_dim,out_channels=embed_dim,kernel_size=1,bias=qkv_bias)\n",
    "        self.proj = nn.Conv1d(in_channels=embed_dim,out_channels=embed_dim,kernel_size=1)\n",
    "    def forward(self,x):\n",
    "        B,T,E = x.shape\n",
    "        q = self.query(x.transpose(1,2)).view(B,self.num_head,E//self.num_head,T).transpose(2,3)\n",
    "        k = self.key(x.transpose(1,2)).view(B,self.num_head,E//self.num_head,T).transpose(2,3)\n",
    "        v = self.value(x.transpose(1,2)).view(B,self.num_head,E//self.num_head,T).transpose(2,3)\n",
    "        atten = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        atten = atten.softmax(dim=-1)\n",
    "        x = (atten @ v).transpose(2,3).reshape(B,E,T)\n",
    "        x = self.proj(x).transpose(1,2)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock (nn.Module):\n",
    "    def __init__(self,embed_dim,num_heads,mlp_ratio=4.0,in_channel=1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        # self.multi = nn.MultiheadAttention(embed_dim=embed_dim,num_heads=num_heads,add_bias_kv=True) #kdim=embed_dim,vdim=embed_dim\n",
    "        self.multi = MultiHeadAttendtion(embed_dim=embed_dim,num_heads=num_heads,qkv_bias=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        hidden_dim = (embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim,hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim,embed_dim)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        # x = self.norm1(x)\n",
    "        # x = x + self.multi(x,x,x)[0]\n",
    "        x = x + self.multi(self.norm1(x))\n",
    "        # print(x.shape)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "class EmbeddingLayer (nn.Module):\n",
    "    def __init__(self,in_channel=1,embed_dim=64,patch_size=128,patch_num=16):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.patch_num = patch_num\n",
    "        self.proj = nn.Linear( in_channel * patch_size**2,embed_dim)\n",
    "        self.dp = nn.Dropout(0.25)\n",
    "        self.token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        # print(x.shape)\n",
    "        x = x.view(batch_size,self.patch_num,-1)\n",
    "        # print(x.shape)\n",
    "        embeding = self.proj(x)\n",
    "        token = self.token.expand(batch_size,-1,-1)\n",
    "        x = self.dp(torch.cat((token,embeding),dim=1))\n",
    "        return x\n",
    "    \n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,in_channel=1,class_num=3,embed_dim=64,depth=4,num_head=4,mlp_ratio=4.0):\n",
    "        super(ViT, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = EmbeddingLayer(in_channel,embed_dim)\n",
    "        self.transformblock = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim,num_head,mlp_ratio,in_channel) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim,class_num)\n",
    "        self.dp = nn.Dropout(0.25)\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        for block in self.transformblock:\n",
    "            x = self.dp(block(x))\n",
    "        token = x[:,0]\n",
    "        token = self.head(self.dp(token))\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ed8ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Cleaned_ViT_data.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ViT(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned_ViT_data.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      4\u001b[0m BASE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mgetcwd()))\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Cleaned_ViT_data.pth'"
     ]
    }
   ],
   "source": [
    "model = ViT(1,3,400,4,16,4).to(device=device)\n",
    "model.load_state_dict(torch.load('DCWreg_ViT.pth',weights_only=False))\n",
    "model.eval()\n",
    "BASE_DIR = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_images = []\n",
    "for phase in os.listdir(os.path.join(BASE_DIR,\"afhq\")):\n",
    "    phase_path = os.path.join(os.path.join(BASE_DIR,\"afhq\"),phase)\n",
    "    for label in os.listdir(phase_path):\n",
    "            label_path = os.path.join(phase_path,label)\n",
    "            rnd_image = os.listdir(label_path)[random.randint(0,len(os.listdir(label_path))-1)]\n",
    "            rnd_image_path = os.path.join(label_path,rnd_image)\n",
    "            image = Image.open(rnd_image_path)\n",
    "            rnd_images.append(image)\n",
    "            # display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Image(image,transform=None):\n",
    "    if transform:\n",
    "        patch = transform(image)\n",
    "    patch_tensor = torch.tensor(np.array(patch),dtype = torch.float32, device=device)\n",
    "    return patch_tensor\n",
    "def classified_image(input,model):\n",
    "    input = input.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        print(output,predicted.item())\n",
    "        classified_image = 'cat' if predicted.item()==0 else 'dog' if predicted.item()==1 else 'wild'\n",
    "        print(f\"The above image is classified as: {classified_image}\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    # transforms.CenterCrop(299),\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72334bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in rnd_images:\n",
    "    display(image)\n",
    "    img_tensor = preprocess_Image(image,transform)\n",
    "    classified_image(img_tensor,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
